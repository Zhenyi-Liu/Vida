{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update conda\n",
    "!conda update -n base -c defaults conda\n",
    "\n",
    "# Install dependencies\n",
    "!conda install --yes --file ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch\n",
    "import torch\n",
    "\n",
    "# Torch Vision\n",
    "import torchvision\n",
    "\n",
    "# Path\n",
    "from pathlib import Path\n",
    "\n",
    "# Matplot\n",
    "import matplotlib.pyplot as pp\n",
    "\n",
    "# Reduce\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "try:\n",
    "    print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "except:\n",
    "    print('CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing <a name=\"pre-processing\"></a>\n",
    "\n",
    "The train dataset is loaded, resized, transformed to tensor and normalized. Data augmentation is applied. The resize and normalization is done to enable the usage of pre-trained models weights.\n",
    "\n",
    "The validation dataset is loaded, resized and normalized.\n",
    "\n",
    "The test dataset is a split of the validation dataset, 80% from images are used to validation and 20% to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(path = None, transform = None):\n",
    "    '''\n",
    "        Load dataset\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        path: str\n",
    "            Dataset path\n",
    "        \n",
    "        transform: torchvision.transforms\n",
    "            Transform function\n",
    "            \n",
    "        Usage\n",
    "        -----\n",
    "        \n",
    "        >>> load(path = '')\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        \n",
    "        Image Folder object\n",
    "        \n",
    "        References\n",
    "        ----------\n",
    "        \n",
    "        https://pytorch.org/docs/stable/torchvision/transforms.html\n",
    "        \n",
    "        https://pytorch.org/docs/stable/data.html\n",
    "    '''\n",
    "    \n",
    "    pth = Path(path)\n",
    "    \n",
    "    if not pth.exists() or not pth.is_dir():\n",
    "        raise Exception('Incompatible path')\n",
    "    \n",
    "    dataset = torchvision.datasets.ImageFolder(\n",
    "        root=path,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def plot(axe = None, images = None, cmap = 'gray', title = '', color = False):\n",
    "    '''\n",
    "        Plot images\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        axe: matplotlib.pyplot.subplots\n",
    "            Matplot buffer\n",
    "        \n",
    "        images: torch.tensor\n",
    "            Tensor matrix\n",
    "            \n",
    "        cmap: str\n",
    "            Color map\n",
    "        \n",
    "        title: str\n",
    "            Matrix title\n",
    "        \n",
    "        color: bool\n",
    "            RGB images\n",
    "            \n",
    "        Usage\n",
    "        -----\n",
    "        \n",
    "        Gray\n",
    "        \n",
    "        >>> figure, axe = pp.subplots(nrows=2, ncols=2, figsize=(2, 2))\n",
    "        >>>\n",
    "        >>> plot(axe, [torch.randn((4, 4)) for image in range(0, 4)])\n",
    "        \n",
    "        RGB\n",
    "        \n",
    "        >>> figure, axe = pp.subplots(nrows=2, ncols=2, figsize=(2, 2))\n",
    "        >>>\n",
    "        >>> plot(axe, [torch.randn((4, 4, 3)) for image in range(0, 4)], color = True)\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        \n",
    "        None\n",
    "        \n",
    "        References\n",
    "        ----------\n",
    "        \n",
    "        https://matplotlib.org/examples/color/colormaps_reference.html\n",
    "    '''\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    if len(axe.shape) == 1:\n",
    "        for col in range(axe.shape[0]):\n",
    "            if color:\n",
    "                axe[col].imshow(images[count].numpy().transpose((1, 2, 0)))\n",
    "            else:\n",
    "                axe[col].imshow(images[count].numpy().squeeze(), cmap=cmap)\n",
    "            \n",
    "            axe[col].axis('off')\n",
    "            \n",
    "            if title:\n",
    "                axe[col].set_title(title)\n",
    "            \n",
    "            count += 1\n",
    "        \n",
    "        return\n",
    "    \n",
    "    for row in range(axe.shape[0]):\n",
    "        for col in range(axe.shape[1]):\n",
    "            if color:\n",
    "                axe[row, col].imshow(images[count].numpy().transpose((1, 2, 0)))\n",
    "            else:\n",
    "                axe[row, col].imshow(images[count].numpy().squeeze(), cmap=cmap)\n",
    "            \n",
    "            axe[row, col].axis('off')\n",
    "            \n",
    "            if title:\n",
    "                axe[row, col].set_title(title)\n",
    "            \n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "height_train, width_train = 224, 224\n",
    "\n",
    "batch_size_train = 102\n",
    "\n",
    "num_workers_train = 0\n",
    "\n",
    "path_train = '../data/train'\n",
    "\n",
    "# Default transform\n",
    "normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "transform_train = [\n",
    "    torchvision.transforms.Resize((height_train, width_train)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    normalize\n",
    "]\n",
    "\n",
    "# Train 1\n",
    "train1 = load(path=path_train, transform=torchvision.transforms.Compose(transform_train))\n",
    "\n",
    "# Train 2\n",
    "_train2 = transform_train.copy()\n",
    "\n",
    "_train2.insert(1, torchvision.transforms.RandomRotation((30, 360)))\n",
    "\n",
    "train2 = load(path=path_train, transform=torchvision.transforms.Compose(_train2))\n",
    "\n",
    "# Train 3\n",
    "_train3 = transform_train.copy()\n",
    "\n",
    "_train3.insert(1, torchvision.transforms.ColorJitter(brightness = 2.0, hue = 0.5, saturation = 0.5))\n",
    "\n",
    "train3 = load(path=path_train, transform=torchvision.transforms.Compose(_train3))\n",
    "\n",
    "# Train 4\n",
    "_train4 = transform_train.copy()\n",
    "\n",
    "_train4.pop(0)\n",
    "\n",
    "_train4.insert(0, torchvision.transforms.RandomCrop(size=(height_train, width_train)))\n",
    "\n",
    "train4 = load(path=path_train, transform=torchvision.transforms.Compose(_train4))\n",
    "\n",
    "# Train 5\n",
    "_train5 = transform_train.copy()\n",
    "\n",
    "_train5.insert(1, torchvision.transforms.RandomHorizontalFlip(p = 1.0))\n",
    "\n",
    "_train5.insert(2, torchvision.transforms.RandomVerticalFlip(p = 1.0))\n",
    "\n",
    "train5 = load(path=path_train, transform=torchvision.transforms.Compose(_train5))\n",
    "\n",
    "# Concat datasets\n",
    "train_vector = [\n",
    "    {'title': 'Original', 'data': train1},\n",
    "    {'title': 'Rotation', 'data': train2},\n",
    "    {'title': 'Brightness', 'data': train3},\n",
    "    {'title': 'Crop', 'data': train4},\n",
    "    {'title': 'Flip', 'data': train5},\n",
    "]\n",
    "\n",
    "for obj in train_vector:\n",
    "    obj['data'] = torch.utils.data.DataLoader(\n",
    "        obj['data'],\n",
    "        batch_size = batch_size_train,\n",
    "        num_workers = num_workers_train,\n",
    "        shuffle = True\n",
    "    )\n",
    "\n",
    "print(reduce(lambda start, length: start + length, [len(obj['data'].dataset) for obj in train_vector]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train dataset\n",
    "%matplotlib inline\n",
    "\n",
    "for obj in train_vector:\n",
    "    # Get epoch\n",
    "    images_train, labels_train = iter(obj['data']).next()\n",
    "    \n",
    "    # Create buffer\n",
    "    figure, axe = pp.subplots(nrows=1, ncols=4, figsize=(10, 10))\n",
    "     \n",
    "    # Plot images\n",
    "    plot(axe = axe, images = images_train[:, 1], title = obj['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation dataset\n",
    "height_validation, width_validation = 224, 224\n",
    "\n",
    "batch_size_validation = 102\n",
    "\n",
    "num_workers_validation = 0\n",
    "\n",
    "path_validation = '../data/valid'\n",
    "# Default transform\n",
    "transform_validation = [\n",
    "    torchvision.transforms.Resize((height_validation, width_validation)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    normalize\n",
    "]\n",
    "\n",
    "# Validation\n",
    "validation = torch.utils.data.DataLoader(\n",
    "    load(path = path_validation, transform=torchvision.transforms.Compose(transform_validation)),\n",
    "    batch_size = batch_size_validation,\n",
    "    num_workers = num_workers_validation,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "print(len(validation.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot validation dataset\n",
    "%matplotlib inline\n",
    "\n",
    "images_validation, labels_validation = iter(validation).next()\n",
    "\n",
    "# Create buffer\n",
    "figure, axe = pp.subplots(nrows=1, ncols=4, figsize=(10, 10))\n",
    "\n",
    "# Plot images\n",
    "plot(axe = axe, images = images_validation[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "validation_size = int(0.8 * len(validation.dataset))\n",
    "\n",
    "test_size = int(len(validation.dataset) - validation_size)\n",
    "\n",
    "validation, test = torch.utils.data.random_split(validation.dataset, [validation_size, test_size])\n",
    "\n",
    "print(len(validation), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usage of pre-trained models require some transformations, see the [documentation](https://pytorch.org/docs/stable/torchvision/models.html#torchvision-models) for more details. The the freezed layers can be found on `~/.torch/models`.\n",
    "\n",
    "To architecture references, read the papers [rethinking the inception architecture for computer vision](https://arxiv.org/abs/1512.00567) and [deep residual learning for image recognition](https://arxiv.org/abs/1512.03385) ([talk](https://www.youtube.com/watch?v=C6tLw-rPQ2o))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resnet 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "resnet18 = torchvision.models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapt the fully connected layer to the dataset\n",
    "resnet18.fc = torch.nn.Linear(512, 102, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process model using GPU\n",
    "if torch.cuda.is_available:\n",
    "    resnet18.cuda()\n",
    "    \n",
    "    print('CUDA')\n",
    "else:\n",
    "    print('CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "resnet18.train()\n",
    "\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(resnet18.parameters())\n",
    "\n",
    "for obj in train_vector:\n",
    "    for epoch, (images, labels) in enumerate(obj['data']):\n",
    "        # Process data using GPU\n",
    "        if torch.cuda.is_available:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            \n",
    "        # Set the gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute the output\n",
    "        output = resnet18(images)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        # Retro propagate\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        print('Epoch [{1}][{0}]: {2} loss'.format(obj['title'].lower(), epoch, loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
