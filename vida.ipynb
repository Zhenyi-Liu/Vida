{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vida.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "KDM9XWgmMAqx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Dependencies"
      ]
    },
    {
      "metadata": {
        "id": "DHNJvBStuhXN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#PyTorch\n",
        "from os.path import exists\n",
        "\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "cuda_output = !ldconfig -p | grep cudart.so | sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "\n",
        "# Pillow\n",
        "!pip install Pillow==4.1.1\n",
        "\n",
        "!pip install image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G0TMWWQx0rEK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !wget -cq https://github.com/udacity/pytorch_challenge/raw/master/cat_to_name.json\n",
        "\n",
        "# !wget -cq https://s3.amazonaws.com/content.udacity-data.com/courses/nd188/flower_data.zip\n",
        "\n",
        "# !unzip -qq flower_data.zip\n",
        "\n",
        "# !rm -f flower_data.zip || true\n",
        "\n",
        "path_train = 'flower_data/train'\n",
        "\n",
        "path_validation = 'flower_data/valid'\n",
        "\n",
        "path_classes = 'cat_to_name.json'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "85vmQalmMAq3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ]
    },
    {
      "metadata": {
        "id": "Pwj8EC4tMAq4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Torch\n",
        "import torch\n",
        "\n",
        "# Torch Vision\n",
        "import torchvision\n",
        "\n",
        "# Path\n",
        "from pathlib import Path\n",
        "\n",
        "# Matplot\n",
        "import matplotlib.pyplot as pp\n",
        "\n",
        "# Reduce\n",
        "from functools import reduce\n",
        "\n",
        "# Load\n",
        "import json\n",
        "\n",
        "# Numpy\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1nirkaMiMAq8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### GPU"
      ]
    },
    {
      "metadata": {
        "id": "-1qJ6jTwMAq9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Check if CUDA is available\n",
        "try:\n",
        "    print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
        "       \n",
        "    # Set default tensor\n",
        "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "except:\n",
        "    print('CPU')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qwfwA0KrMArD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pre-processing <a name=\"pre-processing\"></a>\n",
        "\n",
        "The train dataset is loaded, resized, transformed to tensor and normalized; besides that, another transformations are applied to augment the dataset. The test dataset is a split of the validation dataset, 80% from the dataset is used to validation and 20% to test."
      ]
    },
    {
      "metadata": {
        "id": "92uZ0C-0MArE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load(path = None, transform = None):\n",
        "    '''\n",
        "        Load dataset\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        \n",
        "        path: str\n",
        "            Dataset path\n",
        "        \n",
        "        transform: torchvision.transforms\n",
        "            Transform function\n",
        "            \n",
        "        Usage\n",
        "        -----\n",
        "        \n",
        "        >>> load(path = '')\n",
        "        \n",
        "        Return\n",
        "        ------\n",
        "        \n",
        "        Image Folder\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        \n",
        "        https://pytorch.org/docs/stable/torchvision/transforms.html\n",
        "        \n",
        "        https://pytorch.org/docs/stable/data.html\n",
        "    '''\n",
        "    \n",
        "    pth = Path(path)\n",
        "    \n",
        "    if not pth.exists() or not pth.is_dir():\n",
        "        raise Exception('Incompatible path')\n",
        "    \n",
        "    return torchvision.datasets.ImageFolder(root = path, transform = transform)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_--B1NrrMArI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot(axe = None, images = None, cmap = 'gray', title = '', color = False):\n",
        "    '''\n",
        "        Plot images\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        \n",
        "        axe: matplotlib.pyplot.subplots\n",
        "            Matplot buffer\n",
        "        \n",
        "        images: torch.tensor\n",
        "            Tensor matrix\n",
        "            \n",
        "        cmap: str\n",
        "            Color map\n",
        "        \n",
        "        title: str\n",
        "            Matrix title\n",
        "        \n",
        "        color: bool\n",
        "            RGB images\n",
        "            \n",
        "        Usage\n",
        "        -----\n",
        "        \n",
        "        Gray\n",
        "        \n",
        "        >>> figure, axe = pp.subplots(nrows=2, ncols=2, figsize=(2, 2))\n",
        "        >>>\n",
        "        >>> plot(axe, [torch.randn((4, 4)) for image in range(0, 4)])\n",
        "        \n",
        "        RGB\n",
        "        \n",
        "        >>> figure, axe = pp.subplots(nrows=2, ncols=2, figsize=(2, 2))\n",
        "        >>>\n",
        "        >>> plot(axe, [torch.randn((4, 4, 3)) for image in range(0, 4)], color = True)\n",
        "        \n",
        "        Return\n",
        "        ------\n",
        "        \n",
        "        None\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        \n",
        "        https://matplotlib.org/examples/color/colormaps_reference.html\n",
        "    '''\n",
        "        \n",
        "    if len(axe.shape) == 1:\n",
        "        count = 0\n",
        "        \n",
        "        for col in range(axe.shape[0]):\n",
        "            if color:\n",
        "                axe[col].imshow(images[count].numpy().transpose((1, 2, 0)))\n",
        "            else:\n",
        "                axe[col].imshow(images[count].numpy().squeeze(), cmap=cmap)\n",
        "            \n",
        "            axe[col].axis('off')\n",
        "            \n",
        "            if title:\n",
        "                axe[col].set_title(title)\n",
        "            \n",
        "            count += 1\n",
        "    else:\n",
        "        raise Exception('Invalid shape')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vZQqq_F359zQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Normalize\n",
        "normalize = torchvision.transforms.Normalize(mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cvIg2PvlMArN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Train\n",
        "height_train, width_train = 224, 224\n",
        "\n",
        "batch_size_train = 124  # Resnet 18 (124), VGG 11 (12)\n",
        "\n",
        "num_workers_train = 5\n",
        "\n",
        "transform_train = [\n",
        "    torchvision.transforms.Resize((height_train, width_train)),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    normalize\n",
        "]\n",
        "\n",
        "# Train 1\n",
        "train1 = load(path = path_train, transform = torchvision.transforms.Compose(transform_train))\n",
        "\n",
        "# Train 2\n",
        "_train2 = transform_train.copy()\n",
        "\n",
        "_train2.insert(1, torchvision.transforms.RandomRotation((-30, 30)))\n",
        "\n",
        "train2 = load(path = path_train, transform = torchvision.transforms.Compose(_train2))\n",
        "\n",
        "# Train 3\n",
        "_train3 = transform_train.copy()\n",
        "\n",
        "_train3.insert(1, torchvision.transforms.ColorJitter(brightness = 1.0, hue = 0.5, saturation = 0.5))\n",
        "\n",
        "train3 = load(path = path_train, transform = torchvision.transforms.Compose(_train3))\n",
        "\n",
        "# Train 4\n",
        "_train4 = transform_train.copy()\n",
        "\n",
        "_train4.pop(0)\n",
        "\n",
        "_train4.insert(0, torchvision.transforms.RandomCrop(size=(height_train, width_train)))\n",
        "\n",
        "train4 = load(path = path_train, transform = torchvision.transforms.Compose(_train4))\n",
        "\n",
        "# Train 5\n",
        "_train5 = transform_train.copy()\n",
        "\n",
        "_train5.insert(1, torchvision.transforms.RandomHorizontalFlip(p = 1.0))\n",
        "\n",
        "train5 = load(path = path_train, transform = torchvision.transforms.Compose(_train5))\n",
        "\n",
        "# Train 6\n",
        "_train6 = transform_train.copy()\n",
        "\n",
        "_train6.insert(1, torchvision.transforms.RandomVerticalFlip(p = 1.0))\n",
        "\n",
        "train6 = load(path = path_train, transform = torchvision.transforms.Compose(_train6))\n",
        "\n",
        "# Concat\n",
        "train_dataset = [\n",
        "    {'title': 'Original', 'data': train1},\n",
        "    {'title': 'Brightness', 'data': train3},\n",
        "    {'title': 'Crop', 'data': train4},\n",
        "    {'title': 'Rotation', 'data': train2},\n",
        "    {'title': 'Flip Horizontal', 'data': train5},\n",
        "    {'title': 'Flip Vertical', 'data': train6}\n",
        "]\n",
        "\n",
        "for dataset in train_dataset:\n",
        "    dataset['data'] = torch.utils.data.DataLoader(\n",
        "        dataset['data'],\n",
        "        num_workers = num_workers_train,\n",
        "        batch_size = batch_size_train,\n",
        "        shuffle = True,\n",
        "    )\n",
        "\n",
        "print(reduce(lambda start, length: start + length, [len(dataset['data'].dataset) for dataset in train_dataset]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qFVldNC9MArT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot train\n",
        "%matplotlib inline\n",
        "\n",
        "for dataset in train_dataset:\n",
        "    # Get batch\n",
        "    images_train, labels_train = iter(dataset['data']).next()\n",
        "    \n",
        "    # Create buffer\n",
        "    figure, axe = pp.subplots(nrows = 1, ncols = 5, figsize=(15, 10))\n",
        "     \n",
        "    # Plot images\n",
        "    plot(axe = axe, images = images_train[:, 1], title = dataset['title'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3dVDflsjMArY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Validation\n",
        "height_validation, width_validation = 224, 224\n",
        "\n",
        "batch_size_validation = 124 # Resnet 18 (124), VGG 11 (12)\n",
        "\n",
        "num_workers_validation = 5\n",
        "\n",
        "# Transform\n",
        "transform_validation = [\n",
        "    torchvision.transforms.Resize((height_validation, width_validation)),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    normalize\n",
        "]\n",
        "\n",
        "# Validation\n",
        "validation = torch.utils.data.DataLoader(\n",
        "    load(path = path_validation, transform = torchvision.transforms.Compose(transform_validation)),\n",
        "    batch_size = batch_size_validation,\n",
        "    num_workers = num_workers_validation,\n",
        "    shuffle = True\n",
        ")\n",
        "\n",
        "print(len(validation.dataset))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wt0OV3uNMArc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot validation\n",
        "%matplotlib inline\n",
        "\n",
        "# Get batch\n",
        "images_validation, labels_validation = iter(validation).next()\n",
        "\n",
        "# Create buffer\n",
        "figure, axe = pp.subplots(nrows = 1, ncols = 5, figsize=(15, 10))\n",
        "\n",
        "# Plot images\n",
        "plot(axe = axe, images = images_validation[:, 1], title = 'Original')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RW7U-eyTMArf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Test dataset\n",
        "# validation_size = int(0.8 * len(validation.dataset))\n",
        "\n",
        "# test_size = int(len(validation.dataset) - validation_size)\n",
        "\n",
        "# validation, test = torch.utils.data.random_split(validation.dataset, [validation_size, test_size])\n",
        "\n",
        "# print(len(validation), len(test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r8YOsAHIMArk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Models"
      ]
    },
    {
      "metadata": {
        "id": "_hfB-66TMArl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "\n",
        "1. [Very deep convolutional networks for large-scale image recognition](https://arxiv.org/abs/1409.1556)\n",
        "\n",
        "2. [Deep residual learning for image recognition](https://arxiv.org/abs/1512.03385)\n",
        "\n",
        "3. [Unsupervised representation learning using convolutional and stacked auto-encoders: a domain and cross-domain feature space analysis](https://arxiv.org/abs/1811.00473)"
      ]
    },
    {
      "metadata": {
        "id": "ME2OTOyBWFsa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Classes"
      ]
    },
    {
      "metadata": {
        "id": "-NhnhO5rWJNM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classes = json.load(open(path_classes))\n",
        "\n",
        "print(classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cdVfsFJAMArm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Resnet 18"
      ]
    },
    {
      "metadata": {
        "id": "fAnWIHf7MAro",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create a model\n",
        "resnet18 = torchvision.models.resnet18(pretrained = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yICqiXbfMArr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Update dense layer\n",
        "resnet18.fc = torch.nn.Linear(512, 102, bias = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Fn9blHIVtG8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "# resnet18.load_state_dict(torch.load('./resnet18.pth'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1ESwhd-_MArt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Freeze first layer\n",
        "for parameter in resnet18.layer1.parameters():\n",
        "    parameter.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6-Psprq2MAry",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Activate CUDA\n",
        "if torch.cuda.is_available:\n",
        "    resnet18 = resnet18.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hsnfu8MQMAr5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define criterion\n",
        "resnet18_criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Define optimizer\n",
        "resnet18_optimizer = torch.optim.Adam(params = resnet18.parameters(), lr = 0.001)\n",
        "\n",
        "# Define epochs\n",
        "resnet18_epochs = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1NyujFro3mmm",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Train\n",
        "resnet18.train(mode = True)\n",
        "\n",
        "resnet18_loss = []\n",
        "\n",
        "for dataset in train_dataset:\n",
        "    for epoch in range(resnet18_epochs):\n",
        "        for batch, (images, labels) in enumerate(dataset['data']):\n",
        "            # Activate CUDA\n",
        "            if torch.cuda.is_available():\n",
        "                images, labels = images.cuda(), labels.cuda()\n",
        "\n",
        "            # Set the gradient to zero\n",
        "            resnet18_optimizer.zero_grad()\n",
        "            \n",
        "            # Compute the output\n",
        "            output = resnet18(images)\n",
        "            \n",
        "            # Compute the loss\n",
        "            loss = resnet18_criterion(output, labels)\n",
        "            \n",
        "            resnet18_loss.append(loss)\n",
        "            \n",
        "            # Retro propagate\n",
        "            loss.backward()\n",
        "\n",
        "            resnet18_optimizer.step()\n",
        "            \n",
        "            # Print loss\n",
        "            print('Epoch {0} | Batch {1} | Dataset {2}: {3} Loss'.format(epoch, batch, dataset['title'], loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DWW0bRam4xCY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot train loss\n",
        "pp.style.use('grayscale')\n",
        "\n",
        "pp.plot(resnet18_loss, linestyle = '-.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MNFGZYsuaPPM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Accuracy\n",
        "resnet18_class_correct = [0.] * len(classes)\n",
        "\n",
        "resnet18_class_total = [0.] * len(classes)\n",
        "\n",
        "# Disable gradient\n",
        "with torch.no_grad():\n",
        "    # Disable normalize and dropout\n",
        "    resnet18.eval()\n",
        "    \n",
        "    for count, (images, labels) in enumerate(validation):\n",
        "        if torch.cuda.is_available():\n",
        "            images, labels = images.cuda(), labels.cuda()\n",
        "        \n",
        "        # Compute the output\n",
        "        output = resnet18(images)\n",
        "        \n",
        "        # Get the max probabilities and its indices (classes)\n",
        "        max_values, prediction = torch.max(output, 1)    \n",
        "        \n",
        "        # Compare predictions to true label\n",
        "        correct = prediction.eq(labels.data.view_as(prediction))\n",
        "        \n",
        "        # Transform to 1D\n",
        "        if torch.cuda.is_available():\n",
        "            correct = np.squeeze(correct.cpu().numpy())\n",
        "        else:\n",
        "            correct = np.squeeze(correct.numpy())\n",
        "        \n",
        "        for c in range(len(images)):\n",
        "            label = labels.data[c]\n",
        "            \n",
        "            resnet18_class_correct[label] += correct[c].item()\n",
        "            \n",
        "            resnet18_class_total[label] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GYd5j8setn9n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Model accuracy: {0}'.format(round(100. * np.sum(resnet18_class_correct) / np.sum(resnet18_class_total)), 2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "48qlTaEWVWci",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "# torch.save(resnet18.state_dict(), './resnet18.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CZOXobwKLn9U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### VGG"
      ]
    },
    {
      "metadata": {
        "id": "n_hptIRzNSGC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define model\n",
        "vgg11 = torchvision.models.vgg11_bn(pretrained = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Bwd0LwDYNhj5",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Update dense layer\n",
        "vgg11.fc = torch.nn.Linear(4096, 102, bias = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bhvoL1bLV4WL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "# vgg11.load_state_dict(torch.load('./vgg11.pth'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "FnYFRbFaNhkR",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Activate CUDA\n",
        "if torch.cuda.is_available:\n",
        "    vgg11 = vgg11.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "w5JVsN7kNhka",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define criterion\n",
        "vgg11_criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Define optimizer\n",
        "vgg11_optimizer = torch.optim.SGD(params = vgg11.parameters(), lr = 0.01)  # 0.001\n",
        "\n",
        "# Define epochs\n",
        "vgg11_epochs = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9pyoO-VKOj-g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Train\n",
        "vgg11.train(mode = True)\n",
        "\n",
        "vgg11_loss = []\n",
        "\n",
        "for dataset in train_dataset:\n",
        "    for epoch in range(vgg11_epochs):\n",
        "        for batch, (images, labels) in enumerate(dataset['data']):\n",
        "            # Activate CUDA\n",
        "            if torch.cuda.is_available():\n",
        "                images, labels = images.cuda(), labels.cuda()\n",
        "\n",
        "            # Set the gradient to zero\n",
        "            vgg11_optimizer.zero_grad()\n",
        "            \n",
        "            # Compute the output\n",
        "            output = vgg11(images)\n",
        "            \n",
        "            # Compute the loss\n",
        "            loss = vgg11_criterion(output, labels)\n",
        "            \n",
        "            vgg11_loss.append(loss)\n",
        "            \n",
        "            # Retro propagate\n",
        "            loss.backward()\n",
        "\n",
        "            vgg11_optimizer.step()\n",
        "            \n",
        "            # Print loss\n",
        "            print('Epoch {0} | Batch {1} | Dataset {2}: {3} Loss'.format(epoch, batch, dataset['title'], loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AB0GRC4xPG6w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot train loss\n",
        "pp.style.use('grayscale')\n",
        "\n",
        "pp.plot(vgg11_loss, linestyle = '-.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sPCFzcD7WKPF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Accuracy\n",
        "vgg11_class_correct = [0.] * len(classes)\n",
        "\n",
        "vgg11_class_total = [0.] * len(classes)\n",
        "\n",
        "# Disable gradient\n",
        "with torch.no_grad():\n",
        "    # Disable normalize and dropout\n",
        "    vgg11.eval()\n",
        "    \n",
        "    for count, (images, labels) in enumerate(validation):\n",
        "        if torch.cuda.is_available():\n",
        "            images, labels = images.cuda(), labels.cuda()\n",
        "        \n",
        "        # Compute the output\n",
        "        output = vgg11(images)\n",
        "                \n",
        "        # Get the max probabilities and its indices (classes)\n",
        "        max_values, prediction = torch.max(output, 1)    \n",
        "        \n",
        "        # Compare predictions to true label\n",
        "        correct = prediction.eq(labels.data.view_as(prediction))\n",
        "        \n",
        "        # Transform to 1D\n",
        "        if torch.cuda.is_available():\n",
        "            correct = np.squeeze(correct.cpu().numpy())\n",
        "        else:\n",
        "            correct = np.squeeze(correct.numpy())\n",
        "        \n",
        "        for c in range(len(images)):\n",
        "            label = labels.data[c]\n",
        "            \n",
        "            vgg11_class_correct[label] += correct[c].item()\n",
        "            \n",
        "            vgg11_class_total[label] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GhOSyzUXmOhz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Model accuracy: {0}'.format(round(100. * np.sum(vgg11_class_correct) / np.sum(vgg11_class_total)), 2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mC0R04oWV9Y8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "# torch.save(vgg11.state_dict(), './vgg11.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}