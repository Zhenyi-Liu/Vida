{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vida.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "KDM9XWgmMAqx",
        "zXtg_JoAZ7nU",
        "85vmQalmMAq3",
        "1nirkaMiMAq8",
        "1EwpwzxpxN_y",
        "ME2OTOyBWFsa",
        "cdVfsFJAMArm"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "KDM9XWgmMAqx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Dependencies"
      ]
    },
    {
      "metadata": {
        "id": "DHNJvBStuhXN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#PyTorch\n",
        "from os.path import exists\n",
        "\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "cuda_output = !ldconfig -p | grep cudart.so | sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "\n",
        "# Pillow\n",
        "!pip install Pillow==4.1.1 image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G0TMWWQx0rEK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget -cq https://github.com/udacity/pytorch_challenge/raw/master/cat_to_name.json\n",
        "\n",
        "!wget -cq https://s3.amazonaws.com/content.udacity-data.com/courses/nd188/flower_data.zip\n",
        "\n",
        "!unzip -qq flower_data.zip\n",
        "\n",
        "!rm -f flower_data.zip || true"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zXtg_JoAZ7nU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Path"
      ]
    },
    {
      "metadata": {
        "id": "_76VJb_B86uf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "path_train = 'flower_data/train'\n",
        "\n",
        "path_validation = 'flower_data/valid'\n",
        "\n",
        "path_classes = 'cat_to_name.json'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "85vmQalmMAq3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ]
    },
    {
      "metadata": {
        "id": "Pwj8EC4tMAq4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Json\n",
        "import json\n",
        "\n",
        "# Torch\n",
        "import torch\n",
        "\n",
        "# Torch Vision\n",
        "import torchvision\n",
        "\n",
        "# Numpy\n",
        "import numpy as np\n",
        "\n",
        "# Path\n",
        "from pathlib import Path\n",
        "\n",
        "# Matplot\n",
        "import matplotlib.pyplot as pp\n",
        "\n",
        "# Reduce\n",
        "from functools import reduce"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1nirkaMiMAq8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### GPU"
      ]
    },
    {
      "metadata": {
        "id": "-1qJ6jTwMAq9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Check if CUDA is available\n",
        "try:\n",
        "    print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
        "       \n",
        "    # Set default tensor to CUDA tensor\n",
        "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "except:\n",
        "    print('CPU')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qwfwA0KrMArD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pre-processing\n",
        "\n",
        "The train dataset is loaded, resized, transformed to tensor and normalized; besides that, other transformations are applied to augment the dataset. The test dataset is a split of the validation dataset, 80% from the dataset is used for validation and 20% to test."
      ]
    },
    {
      "metadata": {
        "id": "ZPgJoT_8sC0O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Functions"
      ]
    },
    {
      "metadata": {
        "id": "92uZ0C-0MArE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load(path = None, transform = None):\n",
        "    '''\n",
        "        Load dataset\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        \n",
        "        path: str\n",
        "            Dataset path\n",
        "        \n",
        "        transform: torchvision.transforms\n",
        "            Transform function\n",
        "            \n",
        "        Usage\n",
        "        -----\n",
        "        \n",
        "        >>> load(path = '')\n",
        "        \n",
        "        Return\n",
        "        ------\n",
        "        \n",
        "        Image Folder\n",
        "    '''\n",
        "    \n",
        "    pth = Path(path)\n",
        "    \n",
        "    if not pth.exists() or not pth.is_dir():\n",
        "        raise Exception('Incompatible path')\n",
        "    \n",
        "    return torchvision.datasets.ImageFolder(root = path, transform = transform)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_--B1NrrMArI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot(axe = None, images = None, cmap = 'gray', title = '', color = False):\n",
        "    '''\n",
        "        Plot images\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        \n",
        "        axe: matplotlib.pyplot.subplots\n",
        "            Matplot buffer\n",
        "        \n",
        "        images: torch.tensor\n",
        "            Tensor matrix\n",
        "            \n",
        "        cmap: str\n",
        "            Color map\n",
        "        \n",
        "        title: str\n",
        "            Matrix title\n",
        "        \n",
        "        color: bool\n",
        "            Plot RGB images\n",
        "            \n",
        "        Usage\n",
        "        -----\n",
        "        \n",
        "        Gray\n",
        "        \n",
        "        >>> figure, axe = pp.subplots(nrows = 2, ncols = 2, figsize = (2, 2))\n",
        "        >>>\n",
        "        >>> plot(axe, [torch.randn((4, 4)) for image in range(0, 4)])\n",
        "        \n",
        "        RGB\n",
        "        \n",
        "        >>> figure, axe = pp.subplots(nrows = 2, ncols = 2, figsize = (2, 2))\n",
        "        >>>\n",
        "        >>> plot(axe, [torch.randn((4, 4, 3)) for image in range(0, 4)], color = True)\n",
        "        \n",
        "        Return\n",
        "        ------\n",
        "        \n",
        "        None\n",
        "        \n",
        "        References\n",
        "        ----------\n",
        "        \n",
        "        https://matplotlib.org/examples/color/colormaps_reference.html\n",
        "    '''\n",
        "        \n",
        "    if len(axe.shape) == 1:\n",
        "        count = 0\n",
        "        \n",
        "        for col in range(axe.shape[0]):\n",
        "            image = images[count].cpu().clone()\n",
        "            \n",
        "            if color:\n",
        "                axe[col].imshow(image.numpy().transpose((1, 2, 0)))\n",
        "            else:\n",
        "                axe[col].imshow(image.numpy().squeeze(), cmap=cmap)\n",
        "            \n",
        "            axe[col].axis('off')\n",
        "            \n",
        "            if title:\n",
        "                axe[col].set_title(title)\n",
        "            \n",
        "            count += 1\n",
        "    else:\n",
        "        raise Exception('Invalid shape')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cQ0Dqu0IsIel",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Processing"
      ]
    },
    {
      "metadata": {
        "id": "vZQqq_F359zQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Normalize (normalize the train, test and validation datasets)\n",
        "normalize = torchvision.transforms.Normalize(mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lB1QLCQRwhrX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Train"
      ]
    },
    {
      "metadata": {
        "id": "cvIg2PvlMArN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Train dataset\n",
        "height_train, width_train = 224, 224\n",
        "\n",
        "batch_size_train = 12\n",
        "\n",
        "num_workers_train = 1\n",
        "\n",
        "# Transform template\n",
        "transform_train = [\n",
        "    torchvision.transforms.Resize((height_train, width_train)),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    normalize\n",
        "]\n",
        "\n",
        "# Train 1 - Original\n",
        "train1 = load(path = path_train, transform = torchvision.transforms.Compose(transform_train))\n",
        "\n",
        "# Train 2 - Rotation\n",
        "_train2 = transform_train.copy()\n",
        "\n",
        "_train2.insert(1, torchvision.transforms.RandomRotation((-30, 30)))\n",
        "\n",
        "train2 = load(path = path_train, transform = torchvision.transforms.Compose(_train2))\n",
        "\n",
        "# Train 3 - Color\n",
        "_train3 = transform_train.copy()\n",
        "\n",
        "_train3.insert(1, torchvision.transforms.ColorJitter(brightness = 1.0, hue = 0.5, saturation = 0.5))\n",
        "\n",
        "train3 = load(path = path_train, transform = torchvision.transforms.Compose(_train3))\n",
        "\n",
        "# Train 4 - Crop\n",
        "_train4 = transform_train.copy()\n",
        "\n",
        "_train4.pop(0)\n",
        "\n",
        "_train4.insert(0, torchvision.transforms.RandomCrop(size=(height_train, width_train)))\n",
        "\n",
        "train4 = load(path = path_train, transform = torchvision.transforms.Compose(_train4))\n",
        "\n",
        "# Train 5 - Flip horizontal\n",
        "_train5 = transform_train.copy()\n",
        "\n",
        "_train5.insert(1, torchvision.transforms.RandomHorizontalFlip(p = 1.0))\n",
        "\n",
        "train5 = load(path = path_train, transform = torchvision.transforms.Compose(_train5))\n",
        "\n",
        "# Train 6 - Flip vertical\n",
        "_train6 = transform_train.copy()\n",
        "\n",
        "_train6.insert(1, torchvision.transforms.RandomVerticalFlip(p = 1.0))\n",
        "\n",
        "train6 = load(path = path_train, transform = torchvision.transforms.Compose(_train6))\n",
        "\n",
        "# Concat datasets\n",
        "train_datasets = [\n",
        "    {'title': 'Original', 'data': train1},\n",
        "    {'title': 'Color', 'data': train3},\n",
        "    {'title': 'Crop', 'data': train4},\n",
        "    {'title': 'Rotation', 'data': train2},\n",
        "    {'title': 'Flip Horizontal', 'data': train5},\n",
        "    {'title': 'Flip Vertical', 'data': train6}\n",
        "]\n",
        "\n",
        "for dataset in train_datasets:\n",
        "    dataset['data'] = torch.utils.data.DataLoader(\n",
        "        dataset['data'],\n",
        "        num_workers = num_workers_train,\n",
        "        batch_size = batch_size_train,\n",
        "        shuffle = True,\n",
        "    )\n",
        "\n",
        "print(reduce(lambda start, length: start + length, [len(dataset['data'].dataset) for dataset in train_datasets]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qFVldNC9MArT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot train dataset\n",
        "%matplotlib inline\n",
        "\n",
        "for dataset in train_datasets:\n",
        "    # Get batch\n",
        "    images_train, labels_train = iter(dataset['data']).next()\n",
        "    \n",
        "    # Create buffer\n",
        "    figure, axe = pp.subplots(nrows = 1, ncols = 5, figsize=(15, 10))\n",
        "     \n",
        "    # Plot images\n",
        "    plot(axe = axe, images = images_train[:, 1], title = dataset['title'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z3tqJxoixL2x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Validation"
      ]
    },
    {
      "metadata": {
        "id": "3dVDflsjMArY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Validation dataset\n",
        "height_validation, width_validation = 224, 224\n",
        "\n",
        "batch_size_validation = 12\n",
        "\n",
        "num_workers_validation = 1\n",
        "\n",
        "# Transform template\n",
        "transform_validation = [\n",
        "    torchvision.transforms.Resize((height_validation, width_validation)),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    normalize\n",
        "]\n",
        "\n",
        "# Validation\n",
        "validation = torch.utils.data.DataLoader(\n",
        "    load(path = path_validation, transform = torchvision.transforms.Compose(transform_validation)),\n",
        "    batch_size = batch_size_validation,\n",
        "    num_workers = num_workers_validation,\n",
        "    shuffle = True\n",
        ")\n",
        "\n",
        "print(len(validation.dataset))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wt0OV3uNMArc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot validation dataset\n",
        "%matplotlib inline\n",
        "\n",
        "# Get batch\n",
        "images_validation, labels_validation = iter(validation).next()\n",
        "\n",
        "# Create buffer\n",
        "figure, axe = pp.subplots(nrows = 1, ncols = 5, figsize = (15, 10))\n",
        "\n",
        "# Plot images\n",
        "plot(axe = axe, images = images_validation[:, 1], title = 'Original')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1EwpwzxpxN_y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Test"
      ]
    },
    {
      "metadata": {
        "id": "RW7U-eyTMArf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Test dataset\n",
        "# validation_size = int(0.8 * len(validation.dataset))\n",
        "\n",
        "# test_size = int(len(validation.dataset) - validation_size)\n",
        "\n",
        "# validation, test = torch.utils.data.random_split(validation.dataset, [validation_size, test_size])\n",
        "\n",
        "# print(len(validation), len(test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r8YOsAHIMArk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Models"
      ]
    },
    {
      "metadata": {
        "id": "_hfB-66TMArl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "\n",
        "1. [Very deep convolutional networks for large-scale image recognition](https://arxiv.org/abs/1409.1556)\n",
        "\n",
        "2. [Deep residual learning for image recognition](https://arxiv.org/abs/1512.03385)"
      ]
    },
    {
      "metadata": {
        "id": "SvCtgY7JsPLJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Functions"
      ]
    },
    {
      "metadata": {
        "id": "Cii79QvEsR0U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(model, datasets, criterion, optimizer):\n",
        "    '''\n",
        "        Train model\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        \n",
        "        model: torchvision.models\n",
        "            Neural Network model\n",
        "        \n",
        "        datasets: list\n",
        "            List of datasets\n",
        "        \n",
        "        criterion: torch.nn\n",
        "            Loss function\n",
        "        \n",
        "        optimizer: torch.optim\n",
        "            Gradient optimizer\n",
        "\n",
        "        Usage\n",
        "        -----\n",
        "        \n",
        "        >>> train(\n",
        "        >>>     torchvision.models.resnet18(),\n",
        "        >>>     [],\n",
        "        >>>     torch.nn.CrossEntropyLoss(),\n",
        "        >>>     torch.optim.Adam(params = torchvision.models.resnet18().parameters(), lr = 0.0001)\n",
        "        >>> )\n",
        "        \n",
        "        Return\n",
        "        ------\n",
        "        \n",
        "        List of loss\n",
        "    '''\n",
        "    \n",
        "    _loss = []\n",
        "    \n",
        "    for dataset in datasets:\n",
        "        for batch, (images, labels) in enumerate(dataset['data']):\n",
        "            # Activate CUDA\n",
        "            if torch.cuda.is_available():\n",
        "                images, labels = images.cuda(), labels.cuda()\n",
        "\n",
        "            # Set the gradient to zero\n",
        "            model.zero_grad()\n",
        "            \n",
        "            # Compute the output\n",
        "            output = model(images)\n",
        "            \n",
        "            # Compute the loss\n",
        "            loss = criterion(output, labels)\n",
        "            \n",
        "            _loss.append(loss)\n",
        "            \n",
        "            # Retro propagate\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Print loss\n",
        "            print('Batch {0} | Dataset {1}: {2} Loss'.format(batch, dataset['title'], loss))\n",
        "            \n",
        "    return _loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "llOcaqoZ5V3K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def accuracy(model, dataset, classes):\n",
        "    '''\n",
        "        Test accuracy\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        \n",
        "        model: torchvision.models\n",
        "            Neural Network model\n",
        "        \n",
        "        dataset: torch.utils.data\n",
        "            Test dataset\n",
        "                \n",
        "        classes: dict\n",
        "            Meta data dict\n",
        "\n",
        "        Usage\n",
        "        -----\n",
        "        \n",
        "        >>> accuracy(\n",
        "        >>>     torchvision.models.resnet18(),\n",
        "        >>>     [],\n",
        "        >>>     {},\n",
        "        >>> )\n",
        "        \n",
        "        Return\n",
        "        ------\n",
        "        \n",
        "        Number of correct predictions to each class, total of objects in each class and samples\n",
        "        \n",
        "        Extra\n",
        "        -----\n",
        "        \n",
        "        Calculate the accuracy: round(100. * np.sum(_correct) / np.sum(_total)), 2)\n",
        "    '''\n",
        "    \n",
        "    _correct = [0.] * len(classes)\n",
        "    \n",
        "    _total = [0.] * len(classes)\n",
        "    \n",
        "    _sample = []\n",
        "    \n",
        "    # Disable gradient\n",
        "    with torch.no_grad():\n",
        "        # Disable normalize and dropout\n",
        "        model.eval()\n",
        "\n",
        "        for batch, (images, labels) in enumerate(dataset):\n",
        "            if torch.cuda.is_available():\n",
        "                images, labels = images.cuda(), labels.cuda()\n",
        "\n",
        "            # Compute the output\n",
        "            output = model(images)\n",
        "\n",
        "            # Get the max probability and it's indices (classes)\n",
        "            probability, prediction = torch.max(output, 1)    \n",
        "            \n",
        "            # Get sample\n",
        "            _sample.append({\n",
        "                'image': images[0],\n",
        "                'label': labels[0],\n",
        "                'predict': prediction[0],\n",
        "                'probability': probability[0]\n",
        "            })\n",
        "            \n",
        "            # Compare predictions and truth labels\n",
        "            correct = prediction.eq(labels.data.view_as(prediction))\n",
        "\n",
        "            # Transform to 1D\n",
        "            if torch.cuda.is_available():\n",
        "                correct = np.squeeze(correct.cpu().numpy())\n",
        "            else:\n",
        "                correct = np.squeeze(correct.numpy())\n",
        "            \n",
        "            for _ in range(len(images)):\n",
        "                label = labels.data[_]\n",
        "\n",
        "                _correct[label] += correct[_].item()\n",
        "\n",
        "                _total[label] += 1\n",
        "            \n",
        "        return _correct, _total, _sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PExvLEiS4mQp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def show_predict(axe, sample, classes):\n",
        "    '''\n",
        "        Plot predicted images\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        \n",
        "        axe: matplotlib.pyplot.subplots\n",
        "            Matplot buffer\n",
        "        \n",
        "        sample: list\n",
        "            List of objects (sample)\n",
        "                \n",
        "        classes: dict\n",
        "            Meta data dict\n",
        "\n",
        "        Usage\n",
        "        -----\n",
        "        \n",
        "        >>> figure, axe = pp.subplots(nrows = 2, ncols = 3, figsize=(15, 5))\n",
        "        >>>\n",
        "        >>> axe(\n",
        "        >>>     axe,\n",
        "        >>>     [{\n",
        "        >>>         'image': torch.Tensor(torch.randn((3, 3, 3))),\n",
        "        >>>         'label':  torch.Tensor([1]),\n",
        "        >>>         'predict': torch.Tensor([1]),\n",
        "        >>>         'probability': torch.Tensor([.5])\n",
        "        >>>     }],\n",
        "        >>>     {'1': 'one'},\n",
        "        >>> )\n",
        "        \n",
        "        Return\n",
        "        ------\n",
        "        \n",
        "        None\n",
        "    '''\n",
        "    \n",
        "    count = 0\n",
        "    \n",
        "    for row in range(axe.shape[0]):\n",
        "        for col in range(axe.shape[1]):\n",
        "            # Convert tensor to array and transpose\n",
        "            image = sample[count]['image'].cpu().clone().numpy().transpose((1, 2, 0))\n",
        "            \n",
        "            # Plot image\n",
        "            axe[row, col].imshow(image[:, :, 0], cmap = 'gray')     \n",
        "            \n",
        "            # Set title\n",
        "            try:\n",
        "                label = classes[str(sample[count]['label'].cpu().numpy())]\n",
        "                \n",
        "                predict = classes[str(sample[count]['predict'].cpu().numpy())]\n",
        "            except:\n",
        "                label = 'label not found'\n",
        "                \n",
        "                predict = 'Label not found'\n",
        "            \n",
        "            axe[row, col].set_title(\n",
        "                '{0} - {1}'.format(label.title(), predict.title()),\n",
        "                color = 'green' if label == predict else 'red' \n",
        "            )\n",
        "            \n",
        "            # Configure axe\n",
        "            # axe[row, col].set_ylabel('Probabily {0}'.format(sample[count]['probability'].cpu().numpy()))\n",
        "            \n",
        "            # axe[row, col].barh(classes, probabilities)\n",
        "            \n",
        "            axe[row, col].axis('off')\n",
        "            \n",
        "            count += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ME2OTOyBWFsa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Classes"
      ]
    },
    {
      "metadata": {
        "id": "-NhnhO5rWJNM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classes = json.load(open(path_classes))\n",
        "\n",
        "print(classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cdVfsFJAMArm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Resnet 18"
      ]
    },
    {
      "metadata": {
        "id": "fAnWIHf7MAro",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create the model\n",
        "resnet18 = torchvision.models.resnet18(pretrained = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yICqiXbfMArr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Update dense layer\n",
        "resnet18.fc = torch.nn.Linear(512, 102, bias = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Fn9blHIVtG8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "# resnet18.load_state_dict(torch.load('./resnet18.pth'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1ESwhd-_MArt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Freeze first two layers\n",
        "for layer in [resnet18.layer1, resnet18.layer2]:\n",
        "    for parameter in layer.parameters():\n",
        "        parameter.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6-Psprq2MAry",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Activate CUDA\n",
        "if torch.cuda.is_available:\n",
        "    resnet18 = resnet18.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hsnfu8MQMAr5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define criterion\n",
        "resnet18_criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Define optimizer\n",
        "resnet18_optimizer = torch.optim.Adam(params = resnet18.parameters(), lr = 0.0001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1NyujFro3mmm",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "resnet18.train(mode = True)\n",
        "\n",
        "resnet18_loss = train(resnet18, train_datasets, resnet18_criterion, resnet18_optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DWW0bRam4xCY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot train loss\n",
        "pp.style.use('grayscale')\n",
        "\n",
        "pp.plot(resnet18_loss, linestyle = '-.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MNFGZYsuaPPM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot accuracy\n",
        "resnet18_class_correct, resnet18_class_total, resnet18_sample = accuracy(resnet18, validation, classes)\n",
        "\n",
        "print('Model accuracy: {0}'.format(round(100. * np.sum(resnet18_class_correct) / np.sum(resnet18_class_total)), 2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KrT2GXUvZaMH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot predictions\n",
        "resnet18_figure, resnet18_axe = pp.subplots(nrows = 2, ncols = 3, figsize=(15, 5))\n",
        "\n",
        "show_predict(resnet18_axe, resnet18_sample, classes)        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "aYOS80mmAS6k",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "torch.save(resnet18.state_dict(), './resnet18.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CZOXobwKLn9U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### VGG"
      ]
    },
    {
      "metadata": {
        "id": "n_hptIRzNSGC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create the model\n",
        "vgg11 = torchvision.models.vgg11_bn(pretrained = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Bwd0LwDYNhj5",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Update dense layer\n",
        "vgg11.fc = torch.nn.Linear(4096, 102, bias = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bhvoL1bLV4WL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "# vgg11.load_state_dict(torch.load('./vgg11.pth'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h73BuuaQqqrc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Freeze features\n",
        "for parameter in vgg11.features.parameters():\n",
        "    parameter.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "FnYFRbFaNhkR",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Activate CUDA\n",
        "if torch.cuda.is_available:\n",
        "    vgg11 = vgg11.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "w5JVsN7kNhka",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define criterion\n",
        "vgg11_criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Define optimizer\n",
        "vgg11_optimizer = torch.optim.Adam(params = vgg11.parameters(), lr = 0.0001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9pyoO-VKOj-g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "vgg11.train(mode = True)\n",
        "\n",
        "vgg11_loss = train(vgg11, train_datasets, vgg11_criterion, vgg11_optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AB0GRC4xPG6w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot train loss\n",
        "pp.style.use('grayscale')\n",
        "\n",
        "pp.plot(vgg11_loss, linestyle = '-.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sPCFzcD7WKPF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot accuracy\n",
        "vgg11_class_correct, vgg11_class_total, vgg11_sample = accuracy(vgg11, validation, classes)\n",
        "\n",
        "print('Model accuracy: {0}'.format(round(100. * np.sum(vgg11_class_correct) / np.sum(vgg11_class_total)), 2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FPLnPdJ279M1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot predictions\n",
        "vgg11_figure, vgg11_axe = pp.subplots(nrows = 2, ncols = 3, figsize=(15, 5))\n",
        "\n",
        "show_predict(vgg11_axe, vgg11_sample, classes)        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mC0R04oWV9Y8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "torch.save(vgg11.state_dict(), './vgg11.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}